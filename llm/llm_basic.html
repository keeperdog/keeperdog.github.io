<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>LLM AI 基础 | keeperdog</title>
    <meta name="generator" content="VuePress 1.9.10">
    
    <meta name="description" content="keeperdog">
    <meta name="theme-color" content="#3eaf7c">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    
    <link rel="preload" href="/assets/css/0.styles.6e54da3e.css" as="style"><link rel="preload" href="/assets/js/app.43ae17cf.js" as="script"><link rel="preload" href="/assets/js/2.e9459d84.js" as="script"><link rel="preload" href="/assets/js/1.6a359339.js" as="script"><link rel="preload" href="/assets/js/23.b30a8c26.js" as="script"><link rel="prefetch" href="/assets/js/10.7772999a.js"><link rel="prefetch" href="/assets/js/11.fbb5f897.js"><link rel="prefetch" href="/assets/js/12.7a864a4a.js"><link rel="prefetch" href="/assets/js/13.85cf1ae9.js"><link rel="prefetch" href="/assets/js/14.356766f3.js"><link rel="prefetch" href="/assets/js/15.391b4260.js"><link rel="prefetch" href="/assets/js/16.1518614c.js"><link rel="prefetch" href="/assets/js/17.ce109c8b.js"><link rel="prefetch" href="/assets/js/18.4f3cbe51.js"><link rel="prefetch" href="/assets/js/19.dae9c7c1.js"><link rel="prefetch" href="/assets/js/20.d3a51191.js"><link rel="prefetch" href="/assets/js/21.64a0c85e.js"><link rel="prefetch" href="/assets/js/22.aa8d2bb9.js"><link rel="prefetch" href="/assets/js/24.a2dc6bd6.js"><link rel="prefetch" href="/assets/js/25.f3f0ccb5.js"><link rel="prefetch" href="/assets/js/26.1c73e93f.js"><link rel="prefetch" href="/assets/js/27.e49a5fb6.js"><link rel="prefetch" href="/assets/js/28.f00b5ac0.js"><link rel="prefetch" href="/assets/js/29.950151ac.js"><link rel="prefetch" href="/assets/js/3.58fac34f.js"><link rel="prefetch" href="/assets/js/30.71b081e2.js"><link rel="prefetch" href="/assets/js/31.f28cdf7f.js"><link rel="prefetch" href="/assets/js/32.45506f8a.js"><link rel="prefetch" href="/assets/js/33.78dc621b.js"><link rel="prefetch" href="/assets/js/34.59cccec0.js"><link rel="prefetch" href="/assets/js/35.72006333.js"><link rel="prefetch" href="/assets/js/36.12376a1d.js"><link rel="prefetch" href="/assets/js/37.37cff265.js"><link rel="prefetch" href="/assets/js/38.b6b40b99.js"><link rel="prefetch" href="/assets/js/39.0ce7d52f.js"><link rel="prefetch" href="/assets/js/4.34a0f087.js"><link rel="prefetch" href="/assets/js/40.ed84ab6d.js"><link rel="prefetch" href="/assets/js/41.d72774f1.js"><link rel="prefetch" href="/assets/js/42.f535f8c1.js"><link rel="prefetch" href="/assets/js/43.39cf8017.js"><link rel="prefetch" href="/assets/js/5.d2a097f1.js"><link rel="prefetch" href="/assets/js/6.3bed0312.js"><link rel="prefetch" href="/assets/js/7.b27555b5.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.56c672f1.js">
    <link rel="stylesheet" href="/assets/css/0.styles.6e54da3e.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/2.jpeg" alt="keeperdog" class="logo"> <span class="site-name can-hide">keeperdog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/frontend/" class="nav-link">
  Frontend
</a></div><div class="nav-item"><a href="/llm/" class="nav-link router-link-active">
  LLM
</a></div><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/sharing/" class="nav-link">
  Sharing
</a></div><div class="nav-item"><a href="/speech/" class="nav-link">
  Speech
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/frontend/" class="nav-link">
  Frontend
</a></div><div class="nav-item"><a href="/llm/" class="nav-link router-link-active">
  LLM
</a></div><div class="nav-item"><a href="/resume/" class="nav-link">
  Resume
</a></div><div class="nav-item"><a href="/sharing/" class="nav-link">
  Sharing
</a></div><div class="nav-item"><a href="/speech/" class="nav-link">
  Speech
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span></span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/llm/" aria-current="page" class="sidebar-link">LLM AI 相关例子 demo</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/llm/#提示词工程" class="sidebar-link">提示词工程</a></li><li class="sidebar-sub-header"><a href="/llm/#ai-聊天机器人" class="sidebar-link">AI 聊天机器人</a></li><li class="sidebar-sub-header"><a href="/llm/#rag-langchain-版本" class="sidebar-link">RAG-Langchain 版本</a></li><li class="sidebar-sub-header"><a href="/llm/#rag-langchain-个人知识库助手" class="sidebar-link">RAG-LangChain 个人知识库助手</a></li><li class="sidebar-sub-header"><a href="/llm/#rag-llamaindex-简单版本" class="sidebar-link">RAG-LlamaIndex 简单版本</a></li><li class="sidebar-sub-header"><a href="/llm/#rag-llamaindex-高级版本" class="sidebar-link">RAG-LlamaIndex 高级版本</a></li><li class="sidebar-sub-header"><a href="/llm/#tavily-web-search" class="sidebar-link">Tavily - Web Search</a></li><li class="sidebar-sub-header"><a href="/llm/#metagpt-智能体之简单加减" class="sidebar-link">Metagpt - 智能体之简单加减</a></li><li class="sidebar-sub-header"><a href="/llm/#metagpt-智能体之意图识别" class="sidebar-link">Metagpt - 智能体之意图识别</a></li><li class="sidebar-sub-header"><a href="/llm/#metagpt-智能体之场景细化" class="sidebar-link">Metagpt - 智能体之场景细化</a></li><li class="sidebar-sub-header"><a href="/llm/#metagpt-智能体之回答助手" class="sidebar-link">Metagpt - 智能体之回答助手</a></li><li class="sidebar-sub-header"><a href="/llm/#metagpt-智能体之搜索助手" class="sidebar-link">Metagpt - 智能体之搜索助手</a></li><li class="sidebar-sub-header"><a href="/llm/#xtuner-finetuning模型" class="sidebar-link">XTuner FineTuning模型</a></li></ul></li><li><a href="/llm/llm_basic.html" aria-current="page" class="active sidebar-link">LLM AI 基础</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#什么是rag" class="sidebar-link">什么是RAG</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#rag-核心模块-组件" class="sidebar-link">RAG 核心模块/组件</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#rag-工作流程" class="sidebar-link">RAG 工作流程</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#在-rag-中的-embedding-嵌入是什么" class="sidebar-link">在 RAG 中的 Embedding 嵌入是什么</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#rag-技术面临着几个关键性的技术挑战" class="sidebar-link">RAG 技术面临着几个关键性的技术挑战</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#如何评价一个rag-知识库-做的好不好" class="sidebar-link">如何评价一个Rag（知识库）做的好不好？</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#什么是langchain" class="sidebar-link">什么是LangChain</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#什么是chain链" class="sidebar-link">什么是Chain链</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#什么是向量数据库" class="sidebar-link">什么是向量数据库</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#向量数据库的核心原理是什么-核心技术是什么" class="sidebar-link">向量数据库的核心原理是什么？核心技术是什么</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#什么是mcp" class="sidebar-link">什么是MCP？</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#mcp-的概率-为什么-mcp-最近很火-它最大的优势是什么" class="sidebar-link">MCP 的概率？为什么 MCP 最近很火？它最大的优势是什么？</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#什么是上下文工程-context-prompt" class="sidebar-link">什么是上下文工程 context prompt？</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#为什么有上下文工程-区别-举个具体的栗子🌰" class="sidebar-link">为什么有上下文工程？区别？举个具体的栗子🌰</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#在选用模型时-如何考量" class="sidebar-link">在选用模型时，如何考量</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#大模型输出出现重复和幻觉如何解决" class="sidebar-link">大模型输出出现重复和幻觉如何解决</a></li><li class="sidebar-sub-header"><a href="/llm/llm_basic.html#方案设计-智能体设计" class="sidebar-link">方案设计 - 智能体设计</a></li></ul></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="llm-ai-基础"><a href="#llm-ai-基础" class="header-anchor">#</a> LLM AI 基础</h1> <h2 id="什么是rag"><a href="#什么是rag" class="header-anchor">#</a> 什么是RAG</h2> <p>RAG就是结合信息检索和生成式模型的技术。主要流程包括两个核心环节：</p> <p>检索：基于用户的输入、从外部知识库（如数据库、文档、网页）检索与问题相关的信息。通常使用向量化表示和向量数据库进行语义匹配。将知识库中的文档进行预处理、分块、清洗并转换为向量表示、存储在向量数据库中。常用的如 Faiss、Milvus等向量数据库存储所有文档向量。用户提问后、对问题进行向量化、并在数据库中执行最近邻搜索、找出语义最相近的 N 条内容</p> <p>然后就是增强：也可以说是构建 Prompt</p> <p>1.将检索到的信息作为上下文、输入给生成模型（如 GPT）。</p> <p>2.相比纯生成模型、RAG 能引用真实数据、减少幻觉（胡编乱造，一本正经的胡说八道）</p> <p>最后就是由将增强后的上下文输入到大型语言模型、综合已有上下文生成最终生成最终的回答或内容。</p> <p>一句话总结: RAG = 向量搜索引擎 + 大模型、让 AI 回答更靠谱、减少幻觉</p> <h2 id="rag-核心模块-组件"><a href="#rag-核心模块-组件" class="header-anchor">#</a> RAG 核心模块/组件</h2> <ul><li>文档加载器：从不同来源加载文档</li> <li>文本分割器：将文档分割成适当大小的块</li> <li>嵌入模型：将文本块转换为向量表示</li> <li>向量数据库：存储文档块的向量表示</li> <li>检索器：根据查询检索相关文档</li> <li>大语言模型：结合检索到的上下文生成回答</li></ul> <h2 id="rag-工作流程"><a href="#rag-工作流程" class="header-anchor">#</a> RAG 工作流程</h2> <p><strong>准备阶段</strong></p> <ul><li>加载文档</li> <li>文档分块</li> <li>生成嵌入向量</li> <li>存储到向量数据库</li></ul> <p><strong>查询阶段</strong></p> <ul><li>接收用户问题</li> <li>检索相关文档</li> <li>结合上下文生成回答</li></ul> <p><strong>RAG（检索增强生成）的完整流程可分为5个核心阶段：</strong></p> <ol><li>用户提问</li> <li>数据准备：清洗文档、分块处理（如PDF转文本切片）</li> <li>向量化：使用嵌入模型（如BERT、BGE）将文本转为向量。也就是Embedding 向量化</li> <li>索引存储：向量存入数据库（如Milvus、Faiss、Elasticsearch）。</li> <li>检索增强：用户提问向量化后检索相关文档。也就是构建 Prompt (问题 + 检索内容)</li> <li>生成答案：将检索结果与问题组合输入大模型生成回答。</li></ol> <p><img src="/assets/img/2025-07-10-18-01-20.a29b557d.png" alt="snapshot"></p> <h2 id="在-rag-中的-embedding-嵌入是什么"><a href="#在-rag-中的-embedding-嵌入是什么" class="header-anchor">#</a> 在 RAG 中的 Embedding 嵌入是什么</h2> <p>Embedding是RAG系统的核心组件、Embedding（嵌入）技术本质上是将文本、图像等非结构化数据转换为高维向量的过程。在实际应用中Embedding解决了传统关键词检索的局限性。</p> <p>比如用户询问如何煮奶茶时、传统检索可能无法找到包含'奶茶制作步骤'的文档、因为它们字面上不匹配。而通过Embedding、系统能够理解这两个表达在语义上的相似性、从而返回相关内容。Embedding的工作原理是通过深度学习模型（如BERT、Sentence-Transformers等）将文本映射到768维或更高的向量空间。</p> <p>在RAG系统中、Embedding的核心价值在于建立查询和文档之间的语义桥梁。当系统收到用户问题后、会将其转化为向量、然后在预先索引的文档向量库中寻找最相似的内容、无论它们在字面表达上是否匹配。这种基于语义的检索方式大幅提升了信息获取的准确性和完整性、为生成模型提供了更高质量的上下文信息，从而产生更精准的回答</p> <h2 id="rag-技术面临着几个关键性的技术挑战"><a href="#rag-技术面临着几个关键性的技术挑战" class="header-anchor">#</a> RAG 技术面临着几个关键性的技术挑战</h2> <ul><li>如何设计合理的文档分块策略,在保持文档语义完整性的同时实现高效的检索;</li> <li>检索质量的优化问题,需要不断提高系统对相关文档的召回率和准确率;</li> <li>上下文融合的问题,即如何有效地将多个检索到的文档片段信息进行整合;</li> <li>答案生成的质量问题,系统需要基于检索到的内容生成准确、连贯且符合上下文的回答。</li></ul> <h2 id="如何评价一个rag-知识库-做的好不好"><a href="#如何评价一个rag-知识库-做的好不好" class="header-anchor">#</a> 如何评价一个Rag（知识库）做的好不好？</h2> <p>评估 RAG 系统需综合考虑：</p> <p>检索质量：召回率、精确率、MAP 等。</p> <p>生成质量：准确性、相关度排序、完整性。（人工标注：对 Top-K 结果进行相关性打分（如 1-5 分））</p> <p>端到端效率：检索速度即响应时间、鲁棒性。</p> <p>业务特定指标：如知识时效性、可解释性。</p> <h2 id="什么是langchain"><a href="#什么是langchain" class="header-anchor">#</a> 什么是LangChain</h2> <p>LangChain 是一个开源框架、专为快速构建复杂的大语言模型应用而设计。
简单来说就是它集成和内置了很多我们开发 AI 大模型应用需要的东西、如内置文档加载器、向量数据库、HTTP API 封装、云服务适配器等、让咱们开箱即用、有点像咱们 Java 界的 Spring。</p> <p>它最大的特点是把模型调用、提示词管理、工具使用、记忆管理这些能力模块化了、让开发者可以很方便地把大模型和数据库、搜索引擎、API服务等结合起来，用链式结构组织复杂任务。</p> <p>主要支持复杂任务编排：通过 Chains（链）和 Agents（代理）将多个LLM调用和工具操作组合成工作流</p> <p>以及实现上下文管理Memory（记忆）：通过 Memory 组件（如对话历史缓存、实体关系跟踪）实现长对话连贯性。</p> <h2 id="什么是chain链"><a href="#什么是chain链" class="header-anchor">#</a> 什么是Chain链</h2> <p>在 LangChain 中，Chain（链）是一种将多个组件（如 LLM 调用、API 调用、数据处理步骤等）组合在一起，以实现特定复杂任务的机制 。可以将 Chain 理解为一条有序的任务流水线，它把不同的功能模块串联起来，让它们按照特定顺序协同工作，从而高效地完成复杂任务。</p> <h2 id="什么是向量数据库"><a href="#什么是向量数据库" class="header-anchor">#</a> 什么是向量数据库</h2> <p>向量数据库它可以将非结构化数据(如文本、图片、音频等)转换成高维向量的形式进行存储、通过向量数据库预先存储结构化段、实时检索最相关的 Top-K 内容作为上下文输入、并通过高效的相似性搜索算法、快速找到与目标向量最接近的数据项。</p> <p>而向量数据库针对高维向量数据优化、支持近似最近邻(ANN)搜索算法、更适合语义相似性搜索。相似性度量余弦相似度、欧氏距离可以理解为TopN系列、检索TopK相关内容作为上下文输入。向量数据库预先向量化并建立索引（如 HNSW、IVF），实现亚秒级检索。代表性的向量数据库就是Milvus：一个开源的向量数据库系统，轻量级的chroma等。</p> <h2 id="向量数据库的核心原理是什么-核心技术是什么"><a href="#向量数据库的核心原理是什么-核心技术是什么" class="header-anchor">#</a> 向量数据库的核心原理是什么？核心技术是什么</h2> <p>向量数据库的核心原理是通过将高维数据（如图像、文本）转换为多维向量、并基于相似性度量（如余弦相似度、欧氏距离），利用高效的索引结构和近似最近邻（ANN）算法、快速检索与目标最相似的向量结果。</p> <p>这一过程可概括为三个关键步骤：</p> <p>首先是向量化：我们通过嵌入模型将非结构化数据映射为稠密向量、比如用BERT处理文本、ResNet处理图像、或CLIP处理多模态数据。这些模型能捕获数据的语义或特征信息、通常生成128到2048维的向量</p> <p>其次是索引构建：为了高效检索、我们会采用分层导航小世界图（HNSW）等结构预处理向量。HNSW能将搜索复杂度降至对数级O(log N)。同时我们还会利用乘积量化（PQ）来压缩向量、减少内存占用、以及通过倒排索引（IVF）缩小搜索范围。</p> <p>最后是近似搜索：在实际应用中我们允许一定误差来提升速度。ANN算法会在准确性和效率间寻找平衡点、确保在毫秒级延迟内返回Top-K相似结果、同时保持95%以上的召回率。</p> <p>总的来说就四个核心层：向量化引擎-&gt;索引结构 -&gt;相似度计算-&gt;搜索</p> <p>原始数据 → 向量化 → 索引构建（HNSW/PQ/LSH） → 输入查询向量 → ANN近似搜索 → 返回Top-K结果</p> <h2 id="什么是mcp"><a href="#什么是mcp" class="header-anchor">#</a> 什么是MCP？</h2> <p>MCP 是模型上下文协议（Model Context Protocol）的简称。它是连接不同 AI 模型的 “数字桥梁”，定义了模型间上下文信息的标准化交互规则，旨在解决异构模型间数据格式不兼容、语义理解偏差等问题。MCP 通过标准化模型与外部资源的交互方式，让 AI 应用能动态访问和集成外部资源，如数据库、API、网页内容抓取 等，提升了 LLM 应用的功能性、灵活性和可扩展性</p> <p>MCP 和微服务的 “模块化” 思路相似，但 MCP 是 <strong>“同一进程内的功能拆分”，核心是复用和效率；微服务是“跨进程的业务拆分”</strong>，核心是独立部署和系统弹性。简单说：MCP 更像 “乐高积木”（零件拼在一起组成一个整体），微服务更像 “联合办公”（每个团队独立办公，通过邮件 / 会议协作）。</p> <h2 id="mcp-的概率-为什么-mcp-最近很火-它最大的优势是什么"><a href="#mcp-的概率-为什么-mcp-最近很火-它最大的优势是什么" class="header-anchor">#</a> MCP 的概率？为什么 MCP 最近很火？它最大的优势是什么？</h2> <p><strong>概率解释</strong></p> <p>MCP（模型上下文协议）的“概率”不是统计概率，而是把AI模型的随机输出（如幻觉、错误指令）变成确定结果的能力。</p> <ul><li>比如让AI调用天气API，传统模型可能出错，MCP通过标准化协议确保调用准确，结果100%可靠。</li></ul> <p><strong>为什么很火</strong></p> <ul><li><strong>解决行业痛点</strong>：像“AI时代的USB接口”，统一了AI和各种工具（如Excel、GitHub）的交互标准，不用重复开发适配代码。</li> <li><strong>巨头推动</strong>：OpenAI、微软等支持，开发者生态爆发（GitHub星标超2.5万），落地快。</li></ul> <p><strong>优势</strong><br>
标准化+动态协作</p> <ul><li>一次开发，AI能调用所有工具（如写代码时自动查文档、调编译器）；</li> <li>多工具/AI协同时，上下文不丢失（如“分析数据→生成报告→发邮件”一步通），效率提升10倍以上。</li></ul> <h2 id="什么是上下文工程-context-prompt"><a href="#什么是上下文工程-context-prompt" class="header-anchor">#</a> 什么是上下文工程 context prompt？</h2> <p><strong>context prompt</strong> 可称为上下文提示，是指在与人工智能模型交互时，为了帮助模型更好地理解任务背景和要求，从而生成更准确、相关输出，而提供的额外背景信息或线索。</p> <p>这些信息可以包括主题相关的背景知识、用户偏好、历史对话记录、特定情境描述等。例如，在让模型写一篇报道时，通过 context prompt 告知其事件背景、受众群体、风格要求等信息，能引导模型生成更符合期望的内容。</p> <p>简单说，context prompt 就是通过 “填充具体背景” 让 AI 的输出更 “接地气”，尤其适合需要结合特定情境、历史信息或复杂条件的任务。</p> <p><img src="/assets/img/2025-07-10-18-02-31.6fcbe142.png" alt="snapshot"></p> <h2 id="为什么有上下文工程-区别-举个具体的栗子🌰"><a href="#为什么有上下文工程-区别-举个具体的栗子🌰" class="header-anchor">#</a> 为什么有上下文工程？区别？举个具体的栗子🌰</h2> <p><strong>提示词工程</strong>主要侧重于设计单个提示词以引导模型产生期望输出，而<strong>上下文工程</strong>则是为模型构建一个动态的信息环境，使其能更好地处理复杂任务</p> <p>提示词工程有时能通过 “在提示词中嵌入背景信息” 实现部分上下文提示的效果，但这并不意味着两者可以完全等同。<strong>核心差异在于：</strong> 当任务涉及信息规模扩大、动态变化或复杂交互时，单纯的提示词工程会暴露局限，而上下文工程能更系统地解决这些问题。</p> <p>我们可以通过一个更复杂的场景对比两者的边界：</p> <p><strong>场景：</strong> 让 AI 处理一个跨周的客户投诉跟进</p> <p><strong>假设任务是：</strong> 客户王女士上周投诉商品质量问题，本周又反馈售后维修拖延，需要 AI 帮你生成第三次跟进话术，且要体现对前两次沟通的记忆、当前维修进度，还要符合公司 “不轻易承诺赔偿” 的规则。
用 “提示词工程” 处理：
你需要把所有信息压缩进一个提示词里，比如：</p> <p>“请帮我写一段给王女士的跟进话术。背景：王女士上周三（3 月 1 日）投诉购买的加湿器漏水，第一次沟通时我承诺 24 小时内安排检测；3 月 3 日她再次来电，说检测员未按时上门，我道歉后承诺 3 月 5 日前重新安排；今天（3 月 7 日）她第三次联系，说维修仍未完成，情绪激动。注意：公司规定‘未确认责任前不得提赔偿’，且要体现我们记得她前两次的反馈，安抚情绪的同时说明当前维修进度（师傅已取件，预计 3 月 10 日修好）。”</p> <p>这种方式确实能让 AI 生成话术，但问题在于：</p> <p><strong>信息过载风险：</strong> 如果客户投诉持续 1 个月、涉及 5 次沟通，提示词会变得冗长（可能超过模型上下文窗口限制），导致 AI 漏看关键信息；</p> <p><strong>动态更新低效：</strong> 如果维修进度突然提前到 3 月 8 日，你需要手动修改提示词里的时间，每次信息变化都要重新编辑，成本高；</p> <p><strong>缺乏 “记忆管理”：</strong> 如果客户提到 “上次说的检测报告”，提示词里的信息是平铺的，AI 需要自己从冗长文本中定位 “哪次沟通提到了检测报告”，容易出错。</p> <p>用 “上下文工程” 处理：
此时的 “上下文” 会被拆分为结构化的动态模块，由系统自动管理，比如：</p> <p><strong>长期记忆模块：</strong> 存储客户基础信息（王女士，会员等级 V2，历史购买记录）；</p> <p><strong>短期对话记忆：</strong> 按时间线自动整理前两次沟通摘要（3 月 1 日：投诉漏水→承诺检测；3 月 3 日：投诉拖延→承诺重排）；</p> <p><strong>实时状态模块：</strong> 自动同步维修系统数据（当前进度：3 月 7 日取件，预计 3 月 10 日修好）；</p> <p><strong>规则模块：</strong> 内置公司话术禁忌（“禁用‘赔偿’表述，改用‘优先处理’”）。</p> <p>当你让 AI 生成第三次话术时，系统会自动将这些模块 “组装” 成上下文，无需你手动写长提示词。如果维修进度提前到 3 月 8 日，实时状态模块会自动更新，上下文也随之变化；AI 能直接从 “短期对话记忆” 中调取前两次沟通的关键节点，无需在冗长文本中检索。
总结：提示词工程与上下文工程的核心区别</p> <h3 id="提示词工程与上下文工程的核心区别"><a href="#提示词工程与上下文工程的核心区别" class="header-anchor">#</a> 提示词工程与上下文工程的核心区别</h3> <table><thead><tr><th>维度</th> <th>提示词工程</th> <th>上下文工程</th></tr></thead> <tbody><tr><td>信息处理方式</td> <td>静态嵌入：所有信息一次性写进提示词</td> <td>动态组装：按模块拆分，随任务进展自动更新</td></tr> <tr><td>适用规模</td> <td>适合短文本、简单背景（信息量≤提示词长度）</td> <td>适合长周期、多轮交互（信息量可动态扩展）</td></tr> <tr><td>管理复杂度</td> <td>依赖人工维护（信息变化需手动改提示词）</td> <td>系统自动化管理（模块更新自动同步上下文）</td></tr> <tr><td>核心目标</td> <td>优化 “单次指令” 的清晰度</td> <td>构建 “持续交互” 的信息环境</td></tr></tbody></table> <p>简单说：提示词工程是 “写好一句话”，上下文工程是 “搭好一个场”。当任务简单到 “一句话能说清背景” 时，提示词工程足够用；但当任务需要持续的信息积累、动态更新或跨场景整合时，上下文工程才能让 AI 更 “聪明” 地应对。</p> <h2 id="在选用模型时-如何考量"><a href="#在选用模型时-如何考量" class="header-anchor">#</a> 在选用模型时，如何考量</h2> <ol><li><p><strong>匹配任务难度</strong></p> <ul><li>简单场景（如关键词分类）：用轻量模型（TextCNN、小BERT），快且省资源；</li> <li>复杂场景（如模糊意图识别）：用大模型（BERT、GPT系列），靠强语义理解提升准确率。</li></ul></li> <li><p><strong>看数据多少</strong></p> <ul><li>数据少：直接用预训练模型微调（如用通用BERT改改）；</li> <li>数据多：可训练专属模型，针对性优化效果。</li></ul></li> <li><p><strong>算工程成本</strong></p> <ul><li>要速度（如实时对话）：选小模型或压缩版（如DistilBERT）；</li> <li>算力有限：优先规则+轻量模型组合，别浪费资源。</li></ul></li> <li><p><strong>留扩展空间</strong><br>
选支持增量训练的模型，新增场景时不用重训，加数据微调就行。</p></li></ol> <p>核心：<strong>够用就好，别盲目追大模型，平衡效果和成本</strong>。</p> <h2 id="大模型输出出现重复和幻觉如何解决"><a href="#大模型输出出现重复和幻觉如何解决" class="header-anchor">#</a> 大模型输出出现重复和幻觉如何解决</h2> <p>在大模型生成内容时、出现重复和幻觉是两个常见的问题。重复指的是模型在生成文本时出现内容重复的现象、而幻觉则是指模型生成了看似合理但实际上不真实或不准确的信息。为了解决这两个问题、可以通过微调（fine-tuning）的方法进行优化</p> <p>为了解决这些问题、首先微调是非常有效的手段。</p> <p>首先可以确保用于训练的数据质量、要高质量的真实的信息。我们可以减少模型学到错误的信息。特别是领域特定的微调、能帮助模型更准确地生成内容，避免在特定领域（比如医疗、金融）中产生幻觉。</p> <p>此外在训练过程中引入惩罚机制、比如对模型生成重复或不准确内容进行惩罚、也能够引导模型生成更为多样和真实的内容。</p> <p>另一个有效的策略是使用参数高效微调（PEFT）、通过像LoRA这样的技术、在不改变模型主体结构的情况下调整部分参数、从而提高微调效率并减少幻觉的产生。</p> <p>同时强化学习与人类反馈（RLHF）也是一种非常有用的方法、结合人类的评价、模型可以在生成内容时更符合实际世界的逻辑，降低幻觉的风险。</p> <p>最后检索增强生成（RAG）技术也能够显著提高模型输出的准确性、通过在生成过程中引入外部知识库、确保模型生成的信息更为真实和可靠。</p> <p>总的来说：通过微调、引入惩罚机制、领域特定训练和强化学习等方法、可以有效减少大模型的重复和幻觉问题</p> <h2 id="方案设计-智能体设计"><a href="#方案设计-智能体设计" class="header-anchor">#</a> 方案设计 - 智能体设计</h2> <p>想要做一个智能体，其中这个智能体如何识别用用户输入的问题，然后去分流到不同智能体中，最后得到答案呢？你有没有什么思路或者技术方案</p> <ol><li><strong>输入预处理</strong><br>
快速清洗用户输入（去噪声、标准化格式，如语音转文本用ASR、图片转文本用OCR），统一为文本格式，便于后续处理。</li> <li><strong>意图与分类识别（核心）</strong><br>
用“规则+模型”双轨判断用户问题的领域/类型：</li></ol> <ul><li>简单场景：规则引擎（关键词匹配、正则表达式，如含“天气”→归为天气类）；</li> <li>复杂场景：文本分类模型（如BERT、TextCNN），通过训练数据识别意图（如“推荐餐厅”“数学解题”），同时提取关键实体（如地点、时间）辅助分类。</li></ul> <ol start="3"><li><strong>智能体路由</strong><br>
基于分类结果，调用预设的路由规则库（如“天气类→天气智能体”“计算类→计算器智能体”），动态转发请求到对应专业智能体（支持智能体注册/注销，灵活扩展）。</li> <li><strong>结果聚合与反馈</strong><br>
接收专业智能体的返回结果，统一格式后直接返回；若多智能体参与（如跨领域问题），按优先级/权重融合结果，确保输出简洁。</li></ol> <p><strong>异常处理</strong>：识别失败时，自动提示用户补充信息或转人工兜底，保证流程不中断。</p> <p>这套方案兼顾灵活性（支持场景扩展）和效率（简单问题快速分流，复杂问题精准路由）。</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">7/11/2025, 12:41:06 PM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/llm/" class="prev router-link-active">
        LLM AI 相关例子 demo
      </a></span> <!----></p></div> </main></div><div class="global-ui"><!----></div></div>
    <script src="/assets/js/app.43ae17cf.js" defer></script><script src="/assets/js/2.e9459d84.js" defer></script><script src="/assets/js/1.6a359339.js" defer></script><script src="/assets/js/23.b30a8c26.js" defer></script>
  </body>
</html>
